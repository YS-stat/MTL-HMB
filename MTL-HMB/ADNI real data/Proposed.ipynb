{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdbb59a1-2e2a-4c3f-bcb6-cb436e55ca2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set seeds for reproducibility across torch, numpy, and random.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)  \n",
    "\n",
    "def prepare_data(X, y, train_ratio=0.6, val_ratio=0.2, seed=None):\n",
    "    \"\"\"Split data into training, validation, and testing sets with seed control.\"\"\"\n",
    "    total_size = len(X)\n",
    "    train_size = int(train_ratio * total_size)\n",
    "    val_size = int(val_ratio * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "    dataset = TensorDataset(X, y)\n",
    "    \n",
    "    generator = torch.Generator().manual_seed(seed) if seed is not None else None\n",
    "    return random_split(dataset, [train_size, val_size, test_size], generator=generator)\n",
    "\n",
    "\n",
    "class FlexibleMLP(nn.Module):\n",
    "    \"\"\"Flexible multi-layer perceptron with configurable depth and width.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, depth, width):\n",
    "        super(FlexibleMLP, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(depth):\n",
    "            if i < depth - 1:\n",
    "                layers.append(nn.Sequential(nn.Linear(input_dim, width), nn.ReLU()))\n",
    "                input_dim = width\n",
    "            else:\n",
    "                layers.append(nn.Linear(input_dim, output_dim))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "def imputation_grid_search(X_t_1_train, X_t_1_val, X_s_1_train, X_s_1_val,\n",
    "                           X_t_2_train, X_t_2_val, X_s_1, input_dim, output_dim, seed):\n",
    "    \"\"\"\n",
    "    Grid search over neural network hyperparameters to impute missing features.\n",
    "    Shared encoder + unique encoders + decoder + predictor.\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "    widths = [8, 16, 32]\n",
    "    depths = [1, 2, 3]\n",
    "    lrs = [0.001]\n",
    "    lambda_orths = [0, 1]\n",
    "\n",
    "    best_eval_loss = float(\"inf\")\n",
    "    best_model = None\n",
    "    best_config = None\n",
    "\n",
    "    for width, depth, lr, lambda_orth in itertools.product(widths, depths, lrs, lambda_orths):\n",
    "        shared_encoder = FlexibleMLP(input_dim, width, depth, width)\n",
    "        unique_encoder_t = FlexibleMLP(input_dim, width, depth, width)\n",
    "        unique_encoder_s = FlexibleMLP(input_dim, width, depth, width)\n",
    "        decoder = FlexibleMLP(2 * width, input_dim, depth, input_dim)\n",
    "        predictor = FlexibleMLP(width, output_dim, depth, width)\n",
    "\n",
    "        loss_fn = nn.MSELoss()\n",
    "        optimizer = optim.Adam(\n",
    "            list(shared_encoder.parameters()) +\n",
    "            list(unique_encoder_t.parameters()) +\n",
    "            list(unique_encoder_s.parameters()) +\n",
    "            list(decoder.parameters()) +\n",
    "            list(predictor.parameters()),\n",
    "            lr=lr\n",
    "        )\n",
    "\n",
    "        num_epochs = 5000\n",
    "        patience = 30\n",
    "        num_bad_epochs = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            # Training\n",
    "            optimizer.zero_grad()\n",
    "            shared_encoder.train()\n",
    "            unique_encoder_t.train()\n",
    "            unique_encoder_s.train()\n",
    "            decoder.train()\n",
    "            predictor.train()\n",
    "\n",
    "            f_t_c = shared_encoder(X_t_1_train)\n",
    "            f_s_c = shared_encoder(X_s_1_train)\n",
    "            f_t_u = unique_encoder_t(X_t_1_train)\n",
    "            f_s_u = unique_encoder_s(X_s_1_train)\n",
    "\n",
    "            X_t_1_hat = decoder(torch.cat([f_t_c, f_t_u], dim=1))\n",
    "            X_s_1_hat = decoder(torch.cat([f_s_c, f_s_u], dim=1))\n",
    "            X_t_2_hat = predictor(f_t_c)\n",
    "\n",
    "            loss_X_t_1 = loss_fn(X_t_1_hat, X_t_1_train)\n",
    "            loss_X_s_1 = loss_fn(X_s_1_hat, X_s_1_train)\n",
    "            loss_X_t_2 = loss_fn(X_t_2_hat, X_t_2_train)\n",
    "\n",
    "            orth = torch.norm(f_t_c.t() @ f_t_u) + torch.norm(f_s_c.t() @ f_s_u)\n",
    "            total_loss = loss_X_t_1 + loss_X_s_1 + loss_X_t_2 + lambda_orth * orth\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Validation\n",
    "            shared_encoder.eval()\n",
    "            with torch.no_grad():\n",
    "                f_t_c_val = shared_encoder(X_t_1_val)\n",
    "                X_t_2_hat_val = predictor(f_t_c_val)\n",
    "                eval_loss = loss_fn(X_t_2_hat_val, X_t_2_val)\n",
    "\n",
    "            if eval_loss < best_eval_loss:\n",
    "                best_eval_loss = eval_loss\n",
    "                best_config = [width, depth, lr, lambda_orth]\n",
    "                best_model = {\n",
    "                    'shared_encoder': deepcopy(shared_encoder),\n",
    "                    'predictor': deepcopy(predictor)\n",
    "                }\n",
    "                num_bad_epochs = 0\n",
    "            else:\n",
    "                num_bad_epochs += 1\n",
    "\n",
    "            if num_bad_epochs >= patience:\n",
    "                break\n",
    "\n",
    "    #print(\"Best config:\", best_config)\n",
    "\n",
    "    # Final prediction\n",
    "    shared_encoder = best_model['shared_encoder']\n",
    "    predictor = best_model['predictor']\n",
    "    shared_encoder.eval()\n",
    "    predictor.eval()\n",
    "    with torch.no_grad():\n",
    "        f_s_c = shared_encoder(X_s_1)\n",
    "        X_s_2_hat_best = predictor(f_s_c)\n",
    "\n",
    "    return X_s_2_hat_best\n",
    "\n",
    "\n",
    "class Integmodel(nn.Module):\n",
    "    \"\"\"\n",
    "    Integration model with shared and task-specific encoders.\n",
    "    Applies layered transformations and regularizations to enforce\n",
    "    feature disentanglement and structural alignment.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, input_dim_t, input_dim_s, shared_out_dim, unique_out_dim, depth, p1, p2, p3, seed=None):\n",
    "        super(Integmodel, self).__init__()\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "        self.depth = depth\n",
    "        self.p1 = p1\n",
    "        self.p2 = p2\n",
    "        self.p3 = p3\n",
    "\n",
    "        def create_layers(in_features, out_features, depth):\n",
    "            layers = []\n",
    "            for _ in range(depth):\n",
    "                layers.append(nn.Sequential(nn.Linear(in_features, out_features), nn.ReLU()))\n",
    "                in_features = out_features\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        def create_layers_y(in_features, out_features, depth):\n",
    "            layers = []\n",
    "            for _ in range(depth):\n",
    "                layers.append(nn.Sequential(nn.Linear(in_features, out_features), nn.ReLU()))\n",
    "                in_features = out_features\n",
    "            layers.append(nn.Linear(in_features, 1))  # final prediction layer\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        def create_layers_y_unique(in_features, out_features, g_features, depth):\n",
    "            layers = []\n",
    "            current_in = in_features\n",
    "            for _ in range(depth):\n",
    "                layers.append(nn.Sequential(nn.Linear(current_in, out_features), nn.ReLU()))\n",
    "                current_in = out_features + g_features\n",
    "            layers.append(nn.Linear(current_in, 1))\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        self.shared_encoder = create_layers(input_dim, shared_out_dim, depth)\n",
    "        self.unique_encoder_t = create_layers(input_dim_t, unique_out_dim, depth)\n",
    "        self.unique_encoder_s = create_layers(input_dim_s, unique_out_dim, depth)\n",
    "        self.g = create_layers_y(shared_out_dim + unique_out_dim, shared_out_dim, depth)\n",
    "        self.g_t = create_layers_y_unique(shared_out_dim + unique_out_dim, unique_out_dim, shared_out_dim, depth)\n",
    "        self.g_s = create_layers_y_unique(shared_out_dim + unique_out_dim, unique_out_dim, shared_out_dim, depth)\n",
    "\n",
    "    def forward(self, x_t, x_s):\n",
    "        # Shared features\n",
    "        f_t_c = self.shared_encoder(x_t[:, :self.p1])\n",
    "        f_s_c = self.shared_encoder(x_s[:, :self.p1])\n",
    "\n",
    "        # Task-specific features\n",
    "        f_t_u = self.unique_encoder_t(x_t[:, self.p1:])\n",
    "        f_s_u = self.unique_encoder_s(x_s[:, self.p1:])\n",
    "\n",
    "        g_input_t = torch.cat([f_t_u, f_t_c], dim=1)\n",
    "        g_input_s = torch.cat([f_s_u, f_s_c], dim=1)\n",
    "\n",
    "        h_t = self.g_t[0](g_input_t)\n",
    "        h_s = self.g_s[0](g_input_s)\n",
    "        h_t_o = self.g[0](g_input_t)\n",
    "        h_s_o = self.g[0](g_input_s)\n",
    "\n",
    "        norm_products = 0\n",
    "        for i in range(1, self.depth):\n",
    "            h_t = self.g_t[i](torch.cat([h_t, h_t_o], dim=1))\n",
    "            h_s = self.g_s[i](torch.cat([h_s, h_s_o], dim=1))\n",
    "            h_t_o = self.g[i](h_t_o)\n",
    "            h_s_o = self.g[i](h_s_o)\n",
    "\n",
    "            half_size = h_t_o.size(1)\n",
    "            wprod = (\n",
    "                torch.matmul(self.g_s[i][0].weight[:, -half_size:], self.g[i][0].weight.t()) +\n",
    "                torch.matmul(self.g_t[i][0].weight[:, -half_size:], self.g[i][0].weight.t())\n",
    "            )\n",
    "            norm_products += torch.norm(wprod) ** 2\n",
    "\n",
    "        # Final outputs\n",
    "        y_t = self.g_t[-1](torch.cat([h_t, h_t_o], dim=1)) + self.g[-1](h_t_o)\n",
    "        y_s = self.g_s[-1](torch.cat([h_s, h_s_o], dim=1)) + self.g[-1](h_s_o)\n",
    "\n",
    "        # Final norm regularization\n",
    "        half_size = h_t_o.size(1)\n",
    "        wprod_last = (\n",
    "            torch.matmul(self.g_s[-1].weight[:, -half_size:], self.g[-1].weight.t()) +\n",
    "            torch.matmul(self.g_t[-1].weight[:, -half_size:], self.g[-1].weight.t())\n",
    "        )\n",
    "        norm_products += torch.norm(wprod_last) ** 2\n",
    "\n",
    "        # Orthogonality regularization\n",
    "        orth = torch.norm(f_t_c.t() @ f_t_u) ** 2 + torch.norm(f_s_c.t() @ f_s_u) ** 2\n",
    "\n",
    "        # Robustness penalty (on tail blocks of unique encoders)\n",
    "        rob_penalty = (\n",
    "            torch.sum(self.unique_encoder_t[0][0].weight[:, self.p1 + self.p2:] ** 2) +\n",
    "            torch.sum(self.unique_encoder_s[0][0].weight[:, self.p1:self.p1 + self.p2] ** 2)\n",
    "        )\n",
    "\n",
    "        return y_t, y_s, orth, rob_penalty, norm_products\n",
    "\n",
    "\n",
    "def grid_search_integmodel(X_t_hat, y_t, X_s_hat, y_s, p1, p2, p3, seed):\n",
    "    \"\"\"\n",
    "    Perform grid search to train Integmodel with regularization.\n",
    "    Evaluate performance on validation and test sets.\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Hyperparameter grid\n",
    "    dims = [32, 64, 128]\n",
    "    depths = [2, 3, 4]\n",
    "    lrs = [0.001]\n",
    "    lambda_orths =[0,0.01]#[0.01, 0.1]\n",
    "    lambda_robs = [0,0.01]#[0.01, 0.1]\n",
    "    lambda_reds = [0,0.01]#[0.01, 0.1]\n",
    "    batch_size = 16\n",
    "    num_epochs = 25000\n",
    "\n",
    "    input_dim = p1\n",
    "    input_dim_t = p1 + p2 + p3\n",
    "    input_dim_s = p1 + p2 + p3\n",
    "\n",
    "    # Data split\n",
    "    train_set_t, val_set_t, test_set_t = prepare_data(X_t_hat, y_t)\n",
    "    train_set_s, val_set_s, test_set_s = prepare_data(X_s_hat, y_s)\n",
    "\n",
    "    train_loader_t = DataLoader(train_set_t, batch_size=batch_size, shuffle=True)\n",
    "    val_loader_t = DataLoader(val_set_t, batch_size=batch_size, shuffle=False)\n",
    "    test_loader_t = DataLoader(test_set_t, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_loader_s = DataLoader(train_set_s, batch_size=batch_size, shuffle=True)\n",
    "    val_loader_s = DataLoader(val_set_s, batch_size=batch_size, shuffle=False)\n",
    "    test_loader_s = DataLoader(test_set_s, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    best_overall_val_loss = float(\"inf\")\n",
    "    best_model = None\n",
    "    best_paras = None\n",
    "\n",
    "    # Grid search loop\n",
    "    for dim, depth, lr, lambda_orth, lambda_rob, lambda_red in itertools.product(\n",
    "        dims, depths, lrs, lambda_orths, lambda_robs, lambda_reds\n",
    "    ):\n",
    "        model = Integmodel(input_dim, input_dim_t, input_dim_s,\n",
    "                           shared_out_dim=dim, unique_out_dim=dim,\n",
    "                           depth=depth, p1=p1, p2=p2, p3=p3)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.99)\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        num_bad_epochs = 0\n",
    "        patience = 30\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for (batch_t, batch_s) in zip(train_loader_t, train_loader_s):\n",
    "                X_t, y_t_batch = batch_t\n",
    "                X_s, y_s_batch = batch_s\n",
    "                optimizer.zero_grad()\n",
    "                y_t_pred, y_s_pred, orth, rob, red = model(X_t, X_s)\n",
    "                loss_t = criterion(y_t_pred, y_t_batch)\n",
    "                loss_s = criterion(y_s_pred, y_s_batch)\n",
    "                total_loss = loss_t + loss_s + lambda_orth * orth + lambda_rob * rob + lambda_red * red\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            total_val_loss = 0\n",
    "            val_batch = 0\n",
    "            for (batch_t, batch_s) in zip(val_loader_t, val_loader_s):\n",
    "                X_t, y_t_batch = batch_t\n",
    "                X_s, y_s_batch = batch_s\n",
    "                with torch.no_grad():\n",
    "                    y_t_pred, y_s_pred, _, _, _ = model(X_t, X_s)\n",
    "                    loss_t = criterion(y_t_pred, y_t_batch)\n",
    "                    loss_s = criterion(y_s_pred, y_s_batch)\n",
    "                    total_val_loss += loss_t.item() + loss_s.item()\n",
    "                    val_batch += 1\n",
    "\n",
    "            avg_val_loss = total_val_loss / val_batch\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_model = deepcopy(model)\n",
    "                num_bad_epochs = 0\n",
    "            else:\n",
    "                num_bad_epochs += 1\n",
    "                if num_bad_epochs >= patience:\n",
    "                    break\n",
    "\n",
    "        # Update best model across all settings\n",
    "        if best_val_loss < best_overall_val_loss:\n",
    "            best_overall_val_loss = best_val_loss\n",
    "            best_paras = [dim, depth, lr, lambda_orth, lambda_rob, lambda_red]\n",
    "            best_model_settings = best_model\n",
    "\n",
    "    # Evaluation on test set\n",
    "    print(\"Best config:\", best_paras)\n",
    "    model = best_model_settings\n",
    "    model.eval()\n",
    "    criterion = nn.MSELoss()\n",
    "    test_loss_t = 0.0\n",
    "    test_loss_s = 0.0\n",
    "    rel_error_t = 0.0\n",
    "    rel_error_s = 0.0\n",
    "    test_batch = 0\n",
    "\n",
    "    for (batch_t, batch_s) in zip(test_loader_t, test_loader_s):\n",
    "        X_t, y_t_batch = batch_t\n",
    "        X_s, y_s_batch = batch_s\n",
    "        with torch.no_grad():\n",
    "            y_t_pred, y_s_pred, _, _, _ = model(X_t, X_s)\n",
    "            test_loss_t += torch.sqrt(criterion(y_t_pred, y_t_batch)).item()\n",
    "            test_loss_s += torch.sqrt(criterion(y_s_pred, y_s_batch)).item()\n",
    "            rel_error_t += (criterion(y_t_pred, y_t_batch) / criterion(torch.zeros_like(y_t_batch), y_t_batch)).item()\n",
    "            rel_error_s += (criterion(y_s_pred, y_s_batch) / criterion(torch.zeros_like(y_s_batch), y_s_batch)).item()\n",
    "            test_batch += 1\n",
    "\n",
    "    avg_test_loss_s = test_loss_s / test_batch\n",
    "    avg_test_loss_t = test_loss_t / test_batch\n",
    "    avg_rel_error_s = rel_error_s / test_batch\n",
    "    avg_rel_error_t = rel_error_t / test_batch\n",
    "\n",
    "    return avg_test_loss_s, avg_test_loss_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4da19407-fce0-47e3-aa45-c1062ed4c6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read CSV\n",
    "df = pd.read_csv(\"ADNI_real_data.csv\")\n",
    "is_source = df['domain'] == 'source'\n",
    "is_target = df['domain'] == 'target'\n",
    "\n",
    "# response\n",
    "y_s = torch.tensor(df.loc[is_source, 'y'].values, dtype=torch.float32).view(-1, 1)\n",
    "y_t = torch.tensor(df.loc[is_target, 'y'].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# first block\n",
    "X_1_cols = [col for col in df.columns if col.startswith(\"X_1_\")]\n",
    "X_s_1 = torch.tensor(df.loc[is_source, X_1_cols].values, dtype=torch.float32)\n",
    "X_t_1 = torch.tensor(df.loc[is_target, X_1_cols].values, dtype=torch.float32)\n",
    "\n",
    "# second block\n",
    "X_2_cols = [col for col in df.columns if col.startswith(\"X_2_\")]\n",
    "X_t_2 = torch.tensor(df.loc[is_target, X_2_cols].values, dtype=torch.float32)\n",
    "\n",
    "# third block\n",
    "X_3_cols = [col for col in df.columns if col.startswith(\"X_3_\")]\n",
    "X_s_3 = torch.tensor(df.loc[is_source, X_3_cols].values, dtype=torch.float32)\n",
    "\n",
    "#\n",
    "p1 = 267\n",
    "p2 = 113\n",
    "p3 = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "595d19e4-0c07-47b1-b13f-f567e655e753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best config: [128, 2, 0.001, 0.01, 0.01, 0]\n",
      "Best config: [128, 4, 0.001, 0, 0.01, 0.01]\n",
      "Best config: [128, 2, 0.001, 0, 0, 0.01]\n",
      "Best config: [64, 2, 0.001, 0.01, 0, 0.01]\n",
      "Best config: [64, 4, 0.001, 0, 0, 0]\n",
      "Best config: [128, 2, 0.001, 0, 0.01, 0]\n",
      "Best config: [64, 2, 0.001, 0.01, 0.01, 0]\n",
      "Best config: [128, 2, 0.001, 0, 0.01, 0]\n",
      "Best config: [64, 2, 0.001, 0, 0.01, 0.01]\n",
      "Best config: [128, 2, 0.001, 0, 0, 0.01]\n",
      "Best config: [128, 2, 0.001, 0, 0.01, 0.01]\n",
      "Best config: [128, 4, 0.001, 0, 0, 0]\n",
      "Best config: [128, 4, 0.001, 0, 0, 0]\n",
      "Best config: [128, 3, 0.001, 0, 0, 0.01]\n",
      "Best config: [128, 2, 0.001, 0.01, 0.01, 0]\n",
      "Best config: [32, 4, 0.001, 0.01, 0.01, 0]\n",
      "Best config: [128, 2, 0.001, 0, 0, 0]\n",
      "Best config: [128, 4, 0.001, 0, 0.01, 0]\n",
      "Best config: [128, 2, 0.001, 0, 0, 0]\n",
      "Best config: [32, 4, 0.001, 0, 0, 0]\n",
      "Proposed Results saved to ./Proposed_ADNI_seeds=20.csv\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "num_seeds = 20\n",
    "for seed in range(num_seeds): \n",
    "    set_seed(seed)\n",
    "    start_time = time.time()  \n",
    "    X_t_1_train, X_t_1_val = train_test_split(X_t_1, test_size=0.5, random_state=seed)\n",
    "    X_s_1_train, X_s_1_val = train_test_split(X_s_1, test_size=0.5, random_state=seed)\n",
    "    X_t_2_train, X_t_2_val = train_test_split(X_t_2, test_size=0.5, random_state=seed)\n",
    "    X_s_3_train, X_s_3_val = train_test_split(X_s_3, test_size=0.5, random_state=seed)\n",
    "\n",
    "    # impute X_s_2   \n",
    "    X_s_2_hat_best=imputation_grid_search(X_t_1_train, X_t_1_val, X_s_1_train, \\\n",
    "                                          X_s_1_val, X_t_2_train, X_t_2_val, X_s_1, input_dim=p1, output_dim=p2, seed=seed)\n",
    "\n",
    "    # impute X_t_3\n",
    "    X_t_3_hat_best=imputation_grid_search(X_s_1_train, X_s_1_val, X_t_1_train, \\\n",
    "                                          X_t_1_val, X_s_3_train, X_s_3_val, X_t_1, input_dim=p1, output_dim=p3, seed=seed)\n",
    "\n",
    "    # mtl\n",
    "    X_t_hat=torch.cat([X_t_1, X_t_1, X_t_2, X_t_3_hat_best],dim=1)\n",
    "    X_s_hat=torch.cat([X_s_1, X_s_1, X_s_2_hat_best, X_s_3],dim=1)\n",
    "    average_test_loss_s, average_test_loss_t = grid_search_integmodel(X_t_hat, y_t, X_s_hat, y_s, p1, p2, p3, seed=seed)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    results.append((seed, average_test_loss_s, average_test_loss_t, elapsed_time))\n",
    "\n",
    "df = pd.DataFrame(results, columns=['seed', 'average_test_loss_s', 'average_test_loss_t', 'time'])\n",
    "filename = f\"Proposed_ADNI_seeds={num_seeds}.csv\"\n",
    "filepath = os.path.join(\".\", filename)  \n",
    "df.to_csv(filepath, index=False)\n",
    "print(f\"Proposed Results saved to {filepath}\")  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33bf3cbb-a82c-44a1-a53f-a697aa4d4c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average_test_loss_s    4.189551\n",
      "average_test_loss_t    2.800516\n",
      "dtype: float64\n",
      "average_test_loss_s    1.371926\n",
      "average_test_loss_t    0.556988\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "mean_values = df[['average_test_loss_s', 'average_test_loss_t']].mean()\n",
    "std_values = df[['average_test_loss_s', 'average_test_loss_t']].std()\n",
    "\n",
    "print(mean_values)\n",
    "print(std_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20266590-b88d-4543-8df7-b4a4f8020b05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
